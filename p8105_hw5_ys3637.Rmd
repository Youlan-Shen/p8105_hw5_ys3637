---
title: "p8105_hw5_ys3637"
author: "Youlan Shen"
date: "2022-11-16"
output: github_document
---
## Set up

```{r}
# library all packages that we need at the beginning
library(tidyverse)

set.seed(1)

# default set up
theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 2

```{r}
# read in data from CSV file
homicide_data <- read_csv("Data/Problem_2_data/homicide-data.csv")
# show the first several lines of the original data
homicide_data
```

This dataset contains `r nrow(homicide_data)` rows and `r ncol(homicide_data)` columns, while each row showing a homicide case in 50 large U.S. cities over the past decade. Variables include uid (the case id), reported_date(date the case reported), victim_last, victim_first (victim last and first name), victim_age, victim_sex, city, state, lat (latitude), lon (longitude), and disposition (if the case is closed or open, or closed with arrest or not). There are total `r nrow(homicide_data)` cases, with each row describing case date, location, victim information, and disposition information.

* Then, to create a city_state variable, and summarize within cities to obtain the total number of homicides and the number of unsolved homicides.

```{r}
# Create a city_state variable
homicide_data <- homicide_data %>% 
  janitor::clean_names() %>% 
  mutate(city_state = str_c(city, ", ", state))
# show it
homicide_data %>% 
  select(uid, city, state, city_state)
# summarize
homicide_data_summary <- homicide_data %>% 
  mutate(unsolved = ifelse(disposition == "Closed by arrest", FALSE, TRUE)) %>% 
  group_by(city_state) %>% 
  summarize(n_homicides = n(),
            n_unsolved = sum(unsolved))
homicide_data_summary
```

* For city Baltimore, MD, estimate the proportion of unsolved homicides, save as an R object and use broom::tidy, and pull out the result.

```{r}
# apply the prop test to the Baltimore, MD
prop_test_Bal_MD <- homicide_data_summary %>% 
  filter(city_state == "Baltimore, MD") %>% 
  mutate(prop_test = map2(n_unsolved, n_homicides, ~ prop.test(.x, .y) %>% 
                          broom::tidy())) %>% 
  select(prop_test) %>% 
  unnest(cols = c(prop_test))
# pull the result
prop_test_Bal_MD[c("estimate", "conf.low", "conf.high")]
```

* Run the prop.test for each city

```{r}
# apply the prop test to each city
prop_test_each_city <- homicide_data_summary %>% 
  mutate(prop_test = map2(n_unsolved, n_homicides, ~ prop.test(.x, .y) %>% 
                          broom::tidy())) %>% 
  unnest(cols = c(prop_test))
# select and tidy the result
prop_test_each_city %>% 
  select(city_state, estimate, conf.low, conf.high)
```

* Create a plot that shows the estimates and CIs for each city

```{r}
each_city_estimate <- prop_test_each_city %>% 
  select(city_state, estimate, conf.low, conf.high)
each_city_estimate %>% 
  ggplot(aes(x = fct_reorder(city_state, estimate, max), y = estimate, color = city_state)) + 
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  labs(
    title = "Estimate Homicide Propotion And CI For Each City",
    x = "City And State",
    y = "Estimate Homicide Prop and CI"
  )
```

## Problem 3

* First generate the dataset and apply the t.test to each data, store the p-value

```{r}
# create a function generate normal variable X, and fix sample size n=30
# fix sigma = 5, and mu as an unfixed input
# the output of the function is the mu estimate and the p.value of the
# t.test applied on to the generated variable X
muhat_p_value <- function(n = 30, mu, sigma = 5) {
  
  data <- tibble(
    x <- rnorm(n, mean = mu, sd = sigma),
  )
  
  t.test(data, mu = 0, conf.level = 0.95) %>% 
    broom::tidy() %>% 
    select(estimate, p.value)
}
```

* If Generate 5000 dataset from the model, their mu estimates and p.values

````{r}
rerun(50, muhat_p_value(mu = 0)) %>%
        bind_rows
```

* Repeat for true mu is equal to 1, 2, 3, 4, 5, 6

```{r}
# create a results_df of true mu, and each estimate and p_value
results_df <-
  tibble(true_mu = c(1, 2, 3, 4, 5, 6)) %>% 
  mutate(
    output_lists = map(.x = true_mu, ~rerun(50, muhat_p_value(mu = .x))),
    estimate_dfs = map(output_lists, bind_rows)) %>% 
  select(-output_lists) %>% 
  unnest(estimate_dfs)
results_df
```

* Make a plot showing the proportion of times the null was rejected (the power of the test) on the y axis and the true value of μ on the x axis.

```{r}
# first summary the proportion and then make a plot
results_df %>% 
  mutate(reject = ifelse(p.value < 0.05, TRUE, FALSE)) %>% 
  group_by(true_mu) %>%
  summarize(prop_reject_null = sum(reject) / 50) %>% 
  ggplot(aes(x = true_mu, y = prop_reject_null)) + 
  geom_point() +
  labs(
    title = "Proportion of Times The Null Was Rejected for Each Mu",
    x = "True Mu",
    y = "Proportion of The Null Was Rejected"
  )
```

From the plot, when the true mu increases, the proportion of times the null was rejected also increases, which means, when the sample true mean is more different from the hypothesis mean (the effect size increases), we are more easily to reject the null hypothesis.

* Make a plot showing the average estimate of μ̂  on the y axis and the true value of μ on the x axis. 

*Make a second plot (or overlay on the first) the average estimate of μ̂  only in samples for which the null was rejected on the y axis and the true value of μ on the x axis. Is the sample average of μ̂  across tests for which the null is rejected approximately equal to the true value of μ? Why or why not?



